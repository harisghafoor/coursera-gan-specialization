{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtuicEyI9znr"
   },
   "source": [
    "# Controllable Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzKJcDsE93Ko"
   },
   "source": [
    "### Goals\n",
    "In this notebook, you're going to implement a GAN controllability method using gradients from a classifier. By training a classifier to recognize a relevant feature, you can use it to change the generator's inputs (z-vectors) to make it generate images with more or less of that feature.\n",
    "\n",
    "You will be started you off with a pre-trained generator and classifier, so that you can focus on the controllability aspects. However, in case you would like to train your own classifier, the code for that has been provided as well.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Observe how controllability can change a generator's output.\n",
    "2. Resolve some of the challenges that entangled features pose to controllability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEX6YjIbYLFC"
   },
   "source": [
    "## Getting started!\n",
    "\n",
    "You will start off by importing useful libraries and packages and defining a visualization function. You have also been provided with the generator, noise, and classifier code from earlier assignments. The classifier has the same archicture as the earlier critic (remember that the discriminator/critic is simply a classifier used to classify real and fake).\n",
    "\n",
    "#### CelebA\n",
    "For this notebook, instead of the MNIST dataset, you will be using [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). CelebA is a dataset of annotated celebrity images. Since they are colored (not black-and-white), the images have three channels for red, green, and blue (RGB).\n",
    "\n",
    "![celeba](celeba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qkMrboggA0f"
   },
   "source": [
    "#### Packages and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_10LYXRsrWo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3-fGaBHgF9A"
   },
   "source": [
    "#### Generator and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIiS_GuegGx8"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (CelebA is rgb, so 3 is our default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
    "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples in the batch, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pByBqodzgakN"
   },
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCUcVPcvgbA1"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier Class\n",
    "    Values:\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (CelebA is rgb, so 3 is our default)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=3, n_classes=2, hidden_dim=64):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.make_classifier_block(im_chan, hidden_dim),\n",
    "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
    "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a classifier block; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the classifier: Given an image tensor, \n",
    "        returns an n_classes-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with im_chan channels\n",
    "        '''\n",
    "        class_pred = self.classifier(image)\n",
    "        return class_pred.view(len(class_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKyIEkWnYZ6J"
   },
   "source": [
    "## Specifying Parameters\n",
    "Before you begin training, you need to specify a few parameters:\n",
    "  *   z_dim: the dimension of the noise vector\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   device: the device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GLwMw2v8Vat"
   },
   "outputs": [],
   "source": [
    "z_dim = 64\n",
    "batch_size = 128\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSNXcCTfYVzY"
   },
   "source": [
    "## Train a Classifier (Optional)\n",
    "\n",
    "You're welcome to train your own classifier with this code, but you are provided with a pretrained one later in the code. Feel free to skip this code block, and if you do want to train your own classifier, it is recommended that you initially go through the assignment with the provided classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "HwBG8BGq64OJ",
    "outputId": "d1229862-02cc-4191-9c9f-d346d3ea5e16"
   },
   "outputs": [],
   "source": [
    "def train_classifier(filename):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # You can run this code to train your own classifier, but there is a provided pretrained one.\n",
    "    # If you'd like to use this, just run \"train_classifier(filename)\"\n",
    "    # to train and save a classifier on the label indices to that filename.\n",
    "\n",
    "    # Target all the classes, so that's how many the classifier will learn\n",
    "    label_indices = range(40)\n",
    "\n",
    "    n_epochs = 3\n",
    "    display_step = 500\n",
    "    lr = 0.001\n",
    "    beta_1 = 0.5\n",
    "    beta_2 = 0.999\n",
    "    image_size = 64\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        CelebA(\".\", split='train', download=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    classifier = Classifier(n_classes=len(label_indices)).to(device)\n",
    "    class_opt = torch.optim.Adam(classifier.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    cur_step = 0\n",
    "    classifier_losses = []\n",
    "    # classifier_val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Dataloader returns the batches\n",
    "        for real, labels in tqdm(dataloader):\n",
    "            real = real.to(device)\n",
    "            labels = labels[:, label_indices].to(device).float()\n",
    "\n",
    "            class_opt.zero_grad()\n",
    "            class_pred = classifier(real)\n",
    "            class_loss = criterion(class_pred, labels)\n",
    "            class_loss.backward() # Calculate the gradients\n",
    "            class_opt.step() # Update the weights\n",
    "            classifier_losses += [class_loss.item()] # Keep track of the average classifier loss\n",
    "\n",
    "            ## Visualization code ##\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                class_mean = sum(classifier_losses[-display_step:]) / display_step\n",
    "                print(f\"Step {cur_step}: Classifier loss: {class_mean}\")\n",
    "                step_bins = 20\n",
    "                x_axis = sorted([i * step_bins for i in range(len(classifier_losses) // step_bins)] * step_bins)\n",
    "                sns.lineplot(x_axis, classifier_losses[:len(x_axis)], label=\"Classifier Loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                torch.save({\"classifier\": classifier.state_dict()}, filename)\n",
    "            cur_step += 1\n",
    "\n",
    "# Uncomment the last line to train your own classfier - this line will not work in Coursera.\n",
    "# If you'd like to do this, you'll have to download it and run it, ideally using a GPU \n",
    "# train_classifier(\"filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iu1TcEA3aSSI"
   },
   "source": [
    "## Loading the Pretrained Models\n",
    "You will then load the pretrained generator and classifier using the following code. (If you trained your own classifier, you can load that one here instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OgrLujk_tYDu",
    "outputId": "57924502-e734-46fc-da2e-df18dd807fb3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "gen = Generator(z_dim).to(device)\n",
    "gen_dict = torch.load(\"pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"]\n",
    "gen.load_state_dict(gen_dict)\n",
    "gen.eval()\n",
    "\n",
    "n_classes = 40\n",
    "classifier = Classifier(n_classes=n_classes).to(device)\n",
    "class_dict = torch.load(\"pretrained_classifier.pth\", map_location=torch.device(device))[\"classifier\"]\n",
    "classifier.load_state_dict(class_dict)\n",
    "classifier.eval()\n",
    "print(\"Loaded the models!\")\n",
    "\n",
    "opt = torch.optim.Adam(classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aq53cc1nZgq"
   },
   "source": [
    "## Training\n",
    "Now you can start implementing a method for controlling your GAN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJuga5nC-b3a"
   },
   "source": [
    "#### Update Noise\n",
    "For training, you need to write the code to update the noise to produce more of your desired feature. You do this by performing stochastic gradient ascent. You use stochastic gradient ascent to find the local maxima, as opposed to stochastic gradient descent which finds the local minima. Gradient ascent is gradient descent over the negative of the value being optimized. Their formulas are essentially the same, however, instead of subtracting the weighted value, stochastic gradient ascent adds it; it can be calculated by `new = old + (∇ old * weight)`, where ∇ is the gradient of `old`. You perform stochastic gradient ascent to try and maximize the amount of the feature you want. If you wanted to reduce the amount of the feature, you would perform gradient descent. However, in this assignment you are interested in maximize your feature using gradient ascent, since many features in the dataset are not present much more often than they're present and you are trying to add a feature to the images, not remove.\n",
    "\n",
    "Given the noise with its gradient already calculated through the classifier, you want to return the new noise vector.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hint for <code><font size=\"4\">calculate_updated_noise</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   Remember the equation for gradient ascent: `new = old + (∇ old * weight)`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9WLR8Oy1rxU"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: calculate_updated_noise\n",
    "def calculate_updated_noise(noise, weight):\n",
    "    '''\n",
    "    Function to return noise vectors updated with stochastic gradient ascent.\n",
    "    Parameters:\n",
    "        noise: the current noise vectors. You have already called the backwards function on the target class\n",
    "          so you can access the gradient of the output class with respect to the noise by using noise.grad\n",
    "        weight: the scalar amount by which you should weight the noise gradient\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    new_noise = noise + ( noise.grad * weight)    \n",
    "    #### END CODE HERE ####\n",
    "    return new_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8s2RbF5F3_lL",
    "outputId": "e165d0bb-a937-4ce0-9f78-3a094513487c"
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "# Check that the basic function works\n",
    "opt.zero_grad()\n",
    "noise = torch.ones(20, 20) * 2\n",
    "noise.requires_grad_()\n",
    "fake_classes = (noise ** 2).mean()\n",
    "fake_classes.backward()\n",
    "new_noise = calculate_updated_noise(noise, 0.1)\n",
    "assert type(new_noise) == torch.Tensor\n",
    "assert tuple(new_noise.shape) == (20, 20)\n",
    "assert new_noise.max() == 2.0010\n",
    "assert new_noise.min() == 2.0010\n",
    "assert torch.isclose(new_noise.sum(), torch.tensor(0.4) + 20 * 20 * 2)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it works for generated images\n",
    "opt.zero_grad()\n",
    "noise = get_noise(32, z_dim).to(device).requires_grad_()\n",
    "fake = gen(noise)\n",
    "fake_classes = classifier(fake)[:, 0]\n",
    "fake_classes.mean().backward()\n",
    "noise.data = calculate_updated_noise(noise, 0.01)\n",
    "fake = gen(noise)\n",
    "fake_classes_new = classifier(fake)[:, 0]\n",
    "assert torch.all(fake_classes_new > fake_classes)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tj-c9LT5lIRC"
   },
   "source": [
    "#### Generation\n",
    "Now, you can use the classifier along with stochastic gradient ascent to make noise that generates more of a certain feature. In the code given to you here, you can generate smiling faces. Feel free to change the target index and control some of the other features in the list! You will notice that some features are easier to detect and control than others.\n",
    "\n",
    "The list you have here are the features labeled in CelebA, which you used to train your classifier. If you wanted to control another feature, you would need to get data that is labeled with that feature and train a classifier on that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "kASNj6nLz7kh",
    "outputId": "50c4dfce-5925-4c85-e601-fb92c4ed5299"
   },
   "outputs": [],
   "source": [
    "# First generate a bunch of images with the generator\n",
    "n_images = 8\n",
    "fake_image_history = []\n",
    "grad_steps = 10 # Number of gradient steps to take\n",
    "skip = 2 # Number of gradient steps to skip in the visualization\n",
    "\n",
    "feature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n",
    "\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n",
    "\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n",
    "\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n",
    "\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n",
    "\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n",
    "\n",
    "### Change me! ###\n",
    "target_indices = feature_names.index(\"Smiling\") # Feel free to change this value to any string from feature_names!\n",
    "\n",
    "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
    "for i in range(grad_steps):\n",
    "    opt.zero_grad()\n",
    "    fake = gen(noise)\n",
    "    fake_image_history += [fake]\n",
    "    fake_classes_score = classifier(fake)[:, target_indices].mean()\n",
    "    fake_classes_score.backward()\n",
    "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
    "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmETsfun7bLc"
   },
   "source": [
    "## Entanglement and Regularization\n",
    "You may also notice that sometimes more features than just the target feature change. This is because some features are entangled. To fix this, you can try to isolate the target feature more by holding the classes outside of the target class constant. One way you can implement this is by penalizing the differences from the original class with L2 regularization. This L2 regularization would apply a penalty for this difference using the L2 norm and this would just be an additional term on the loss function.\n",
    "\n",
    "Here, you'll have to implement the score function: the higher, the better. The score is calculated by adding the target score and a penalty -- note that the penalty is meant to lower the score, so it should have a negative value.\n",
    "\n",
    "For every non-target class, take the difference between the current noise and the old noise. The greater this value is, the more features outside the target have changed. You will calculate the magnitude of the change, take the mean, and negate it. Finally, add this penalty to the target score. The target score is the mean of the target class in the current noise.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">get_score</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   The higher the score, the better!\n",
    "2.   You want to calculate the loss per image, so you'll need to pass a dim argument to [`torch.norm`](https://pytorch.org/docs/stable/generated/torch.norm.html).\n",
    "3.   Calculating the magnitude of the change requires you to take the norm of the difference between the classifications, not the difference of the norms.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabLcvEL7X-J"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_score\n",
    "def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n",
    "    '''\n",
    "    Function to return the score of the current classifications, penalizing changes\n",
    "    to other classes with an L2 norm.\n",
    "    Parameters:\n",
    "        current_classifications: the classifications associated with the current noise\n",
    "        original_classifications: the classifications associated with the original noise\n",
    "        target_indices: the index of the target class\n",
    "        other_indices: the indices of the other classes\n",
    "        penalty_weight: the amount that the penalty should be weighted in the overall score\n",
    "    '''\n",
    "    # Steps: 1) Calculate the change between the original and current classifications (as a tensor)\n",
    "    #           by indexing into the other_indices you're trying to preserve, like in x[:, features].\n",
    "    #        2) Calculate the norm (magnitude) of changes per example.\n",
    "    #        3) Multiply the mean of the example norms by the penalty weight. \n",
    "    #           This will be your other_class_penalty.\n",
    "    #           Make sure to negate the value since it's a penalty!\n",
    "    #        4) Take the mean of the current classifications for the target feature over all the examples.\n",
    "    #           This mean will be your target_score.\n",
    "    #### START CODE HERE ####\n",
    "    other_distances = current_classifications[:,other_indices] - original_classifications[:,other_indices]\n",
    "    # Calculate the norm (magnitude) of changes per example and multiply by penalty weight\n",
    "    other_class_penalty = -torch.norm(other_distances, dim=1).mean() * penalty_weight\n",
    "    # Take the mean of the current classifications for the target feature\n",
    "    target_score = current_classifications[:, target_indices].mean()\n",
    "    #### END CODE HERE ####\n",
    "    return target_score + other_class_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5-vTjn__EKQT",
    "outputId": "f48d4b7e-f9bc-403f-822d-a222f868ebd4"
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "assert torch.isclose(\n",
    "    get_score(torch.ones(4, 3), torch.zeros(4, 3), [0], [1, 2], 0.2), \n",
    "    1 - torch.sqrt(torch.tensor(2.)) * 0.2\n",
    ")\n",
    "rows = 10\n",
    "current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
    "original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
    "\n",
    "# Must be 3\n",
    "assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3\n",
    "\n",
    "current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
    "original_class = torch.tensor([[4] * rows, [4] * rows, [2] * rows, [1] * rows]).T.float()\n",
    "\n",
    "# Must be 3 - 0.2 * sqrt(10)\n",
    "assert torch.isclose(get_score(current_class, original_class, [1, 3] , [0, 2], 0.2), \n",
    "                     -torch.sqrt(torch.tensor(10.0)) * 0.2 + 3)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkrGr-NUGwC8"
   },
   "source": [
    "In the following block of code, you will run the gradient ascent with this new score function. You might notice a few things after running it: \n",
    "\n",
    "1.   It may fail more often at producing the target feature when compared to the original approach. This suggests that the model may not be able to generate an image that has the target feature without changing the other features. This makes sense! For example, it may not be able to generate a face that's smiling but whose mouth is NOT slightly open. This may also expose a limitation of the generator. \n",
    "Alternatively, even if the generator can produce an image with the intended features, it might require many intermediate changes to get there and may get stuck in a local minimum.\n",
    "\n",
    "2.   This process may change features which the classifier was not trained to recognize since there is no way to penalize them with this method. Whether it's possible to train models to avoid changing unsupervised features is an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "colab_type": "code",
    "id": "l3SshFjn-soX",
    "outputId": "4d97c409-589c-46b7-97b3-8d0483e968d5"
   },
   "outputs": [],
   "source": [
    "fake_image_history = []\n",
    "### Change me! ###\n",
    "target_indices = feature_names.index(\"Smiling\") # Feel free to change this value to any string from feature_names from earlier!\n",
    "other_indices = [cur_idx != target_indices for cur_idx, _ in enumerate(feature_names)]\n",
    "noise = get_noise(n_images, z_dim).to(device).requires_grad_()\n",
    "original_classifications = classifier(gen(noise)).detach()\n",
    "for i in range(grad_steps):\n",
    "    opt.zero_grad()\n",
    "    fake = gen(noise)\n",
    "    fake_image_history += [fake]\n",
    "    fake_score = get_score(\n",
    "        classifier(fake), \n",
    "        original_classifications,\n",
    "        target_indices,\n",
    "        other_indices,\n",
    "        penalty_weight=0.1\n",
    "    )\n",
    "    fake_score.backward()\n",
    "    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\n",
    "show_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts.generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(input_dim=1, im_chan=1, hidden_dim=16)\n",
    "gen(torch.randn(3).type(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eloise-em/miniconda3/envs/ssl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/eloise-em/miniconda3/envs/ssl/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/eloise-em/miniconda3/envs/ssl/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/eloise-em/miniconda3/envs/ssl/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <CDAC6E34-8608-3E70-8B2F-32BCD38E90FB> /Users/eloise-em/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "from torchvision import transforms  # type: ignore\n",
    "from torchvision.datasets import MNIST  # type: ignore\n",
    "from torchvision.utils import make_grid  # type: ignore\n",
    "from torch.utils.data import DataLoader  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import os\n",
    "torch.manual_seed(0)  # Set for our testing purposes, please do not change!\n",
    "from scripts.generator import Generator\n",
    "from scripts.discriminator import Discriminator\n",
    "from scripts.utils import (\n",
    "    show_tensor_images,\n",
    "    get_one_hot_labels,\n",
    "    combine_vectors,\n",
    "    get_input_dimensions,\n",
    "    get_noise,\n",
    "    weights_init,\n",
    "    test_input_dims,\n",
    ")\n",
    "from scripts.config import Params\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x34b52f3d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(real[100][0].detach().cpu().numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4889,  0.7452,  1.2671,  ..., -0.0699,  0.1345, -0.3276],\n",
       "        [-0.2475,  0.3197,  0.5707,  ..., -2.3627,  1.2971,  0.1421],\n",
       "        [-1.0722,  0.4915,  0.0036,  ..., -0.3262, -1.2627, -0.6441],\n",
       "        ...,\n",
       "        [ 0.4326, -1.7708, -2.0518,  ...,  0.0631, -2.4129,  0.4334],\n",
       "        [ 1.7277,  0.7238, -0.5597,  ..., -0.8830,  1.7388,  1.5315],\n",
       "        [-0.5749,  1.4879,  2.0396,  ..., -1.0218,  0.5651,  0.2428]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(device)\n",
    "get_noise(cur_batch_size, z_dim, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 74]) <class 'torch.Tensor'> torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 74]) <class 'torch.Tensor'> torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def train(\n",
    "#     n_epochs,\n",
    "#     z_dim,\n",
    "#     display_step,\n",
    "#     device,\n",
    "#     n_classes,\n",
    "#     mnist_shape,\n",
    "#     gen,\n",
    "#     disc,\n",
    "#     writer,\n",
    "#     gen_opt,\n",
    "#     disc_opt,\n",
    "#     criterion,\n",
    "#     dataloader,\n",
    "# ):\n",
    "\n",
    "n_epochs=params.n_epochs\n",
    "z_dim=params.z_dim\n",
    "display_step=params.display_step\n",
    "device=params.device\n",
    "n_classes=params.n_classes\n",
    "mnist_shape=params.mnist_shape\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "# UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        one_hot_labels = (\n",
    "            get_one_hot_labels(labels.to(device), n_classes).squeeze(0).to(device)\n",
    "        )\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "            1, 1, mnist_shape[1], mnist_shape[2]\n",
    "        )\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device).to(device)\n",
    "\n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "\n",
    "        #### START CODE HERE ####\n",
    "        noise_and_labels = combine_vectors(x=fake_noise, y=one_hot_labels).type(\n",
    "            torch.float32\n",
    "        )\n",
    "        print(\n",
    "            noise_and_labels.shape, type(noise_and_labels), noise_and_labels.dtype\n",
    "        )\n",
    "        break\n",
    "        fake = gen(noise_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "\n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels,\n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "\n",
    "        #### START CODE HERE ####\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(x=fake, y=image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(x=real, y=image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        # Save metrics\n",
    "        writer.add_scalar(\"Loss/mean_discriminator_loss\", discriminator_losses[-1], cur_step)  # type: ignore\n",
    "        writer.add_scalar(\"Loss/mean_generator_loss\", generator_losses[-1], cur_step)  # type: ignore\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            fake_fig = show_tensor_images(fake)\n",
    "            real_fig = show_tensor_images(real)\n",
    "            writer.add_figure(f\"Images/Fake Images\", fake_fig, global_step=cur_step)  # type: ignore\n",
    "            writer.add_figure(f\"Images/Real Imags\", real_fig, global_step=cur_step)  # type: ignore\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Generator Loss\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(discriminator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Discriminator Loss\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\n",
    "                \"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\"\n",
    "            )\n",
    "        cur_step += 1\n",
    "writer.close()\n",
    "# return gen, disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::slow_conv_transpose2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m display(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m74\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/coursera-gan-specialization/C1 - Build Basic Generative Adversarial Networks/Week 4/scripts/generator.py:65\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mFunction for completing a forward pass of the generator: Given a noise tensor, \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mreturns generated images.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    noise: a noise tensor with dimensions (n_samples, input_dim)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(noise), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::slow_conv_transpose2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "display(device)\n",
    "gen(torch.randn(1, 74).type(torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "Device used for training: mps\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "params = Params()\n",
    "assert tuple(combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5)).shape) == (\n",
    "    1,\n",
    "    12,\n",
    "    5,\n",
    ")\n",
    "assert tuple(\n",
    "    combine_vectors(\n",
    "        torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()\n",
    "    ).shape\n",
    ") == (1, 30, 12)\n",
    "print(\"Success!\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "print(\"Device used for training:\", device)  # type: ignore\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    MNIST(params.data_path, download=False, transform=transform),\n",
    "    batch_size=params.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(\n",
    "    params.z_dim, params.mnist_shape, params.n_classes\n",
    ")\n",
    "test_input_dims()\n",
    "print(\"Success!\")\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=params.lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=params.lr)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n",
    "writer = SummaryWriter(log_dir=params.log_dir)\n",
    "# gen,disc = train(\n",
    "#     n_epochs=params.n_epochs,\n",
    "#     z_dim=params.z_dim,\n",
    "#     display_step=params.display_step,\n",
    "#     device=params.device,\n",
    "#     n_classes=params.n_classes,\n",
    "#     mnist_shape=params.mnist_shape,\n",
    "#     gen = gen,\n",
    "#     disc = disc,\n",
    "#     writer = writer,\n",
    "#     gen_opt = gen_opt,\n",
    "#     disc_opt = disc_opt,\n",
    "#     criterion = criterion,\n",
    "#     dataloader = dataloader\n",
    "# )\n",
    "# torch.save(gen.state_dict(), params.save_models_directory + \"/mnist_cgan_generator.pth\")\n",
    "# torch.save(disc.state_dict(), params.save_models_directory + \"/mnist_cgan_discriminator.pth\")\n",
    "# print(\"Models have been saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps:0\")\n",
    "gen(torch.randn(size=[128,74]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_and_labels = noise_and_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(74, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 74]) <class 'torch.Tensor'> torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps:0\")\n",
    "cur_batch_size = len(real)\n",
    "# Flatten the batch of real images from the dataset\n",
    "real = real.to(device)\n",
    "\n",
    "one_hot_labels = (\n",
    "    get_one_hot_labels(labels.to(device), n_classes).squeeze(0).to(device)\n",
    ")\n",
    "image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "image_one_hot_labels = image_one_hot_labels.repeat(\n",
    "    1, 1, mnist_shape[1], mnist_shape[2]\n",
    ")\n",
    "### Update discriminator ###\n",
    "# Zero out the discriminator gradients\n",
    "disc_opt.zero_grad()\n",
    "# Get noise corresponding to the current batch_size\n",
    "fake_noise = get_noise(cur_batch_size, z_dim, device=device).to(device)\n",
    "\n",
    "# Now you can get the images from the generator\n",
    "# Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "#        2) Generate the conditioned fake images\n",
    "\n",
    "#### START CODE HERE ####\n",
    "noise_and_labels = combine_vectors(x=fake_noise, y=one_hot_labels).type(\n",
    "    torch.float32\n",
    ")\n",
    "print(\n",
    "    noise_and_labels.shape, type(noise_and_labels), noise_and_labels.dtype\n",
    ")\n",
    "fake = gen(noise_and_labels)\n",
    "# gen.to(device)(noise_and_labels)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wEX6YjIbYLFC",
    "pByBqodzgakN"
   ],
   "name": "C1W4_4: Controllable Generation (Student).ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "GANSC1-4B"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
